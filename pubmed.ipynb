{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pmc.ncbi.nlm.nih.gov/articles/PMC8374293/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Entrez\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os  \n",
    "\n",
    "Entrez.email = \"yilianz@uci.edu\"\n",
    "\n",
    "# Rename disease names for recorded files \n",
    "def sanitize_filename(name):\n",
    "    \n",
    "    return \"\".join(c if c.isalnum() or c in (\"-\", \"_\") else \"_\" for c in name).strip()\n",
    "\n",
    "# Separate list of diseases into n piese \n",
    "def split_list(lst, n):\n",
    "    \n",
    "    k, m = divmod(len(lst), n)\n",
    "    return [lst[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n)]\n",
    "\n",
    "\n",
    "\n",
    "def search_and_save_all_pubmed(disease, filename, json_filename, abnormal_file, start_year=2015):\n",
    "\n",
    "    # set up base search term\n",
    "    base_search_term = (\n",
    "    \"(electronic health record OR electronic health records OR electronic medical record OR electronic medical records OR EHR OR EMR) \"\n",
    "    \"AND ({rare_disease}) \"\n",
    "    #\"AND (large language model OR language model OR NLP OR natural language processing OR language processing OR machine learning OR artificial intelligence OR predictive modeling OR deep learning)\"\n",
    ")\n",
    "    search_term = base_search_term.format(rare_disease=f'{disease.lower()}')\n",
    "    \n",
    "    date_range = f\"{start_year}/01/01:3000/12/31[dp]\"  \n",
    "    search_query = f\"{search_term} AND {date_range}\"\n",
    "\n",
    "    \n",
    "    with Entrez.esearch(db=\"pubmed\", term=search_query, retmax=100000, usehistory=\"y\") as search_handle:\n",
    "        search_results = Entrez.read(search_handle)\n",
    "    \n",
    "    total_count = int(search_results[\"Count\"])\n",
    "    #print(f\"Search term: '{search_query}'\")\n",
    "    #print(f\"Total results found: {total_count}\")\n",
    "\n",
    "\n",
    "    # Append research reults number to Json \n",
    "    if os.path.exists(json_filename):\n",
    "        with open(json_filename, \"r\", encoding=\"utf-8\") as json_file:\n",
    "            existing_data = json.load(json_file)\n",
    "    else:\n",
    "        existing_data = {\"true_count\": 0,\"count\": 0, \"results\": []}\n",
    "\n",
    "    existing_data[\"results\"].append({\"disease_name\": disease,\"search_query\": search_query, \"count\": total_count})\n",
    "\n",
    "    # Get Abnormal count out of calculations, specified how many total research achived and how many real we need  \n",
    "    if total_count >= 1000:\n",
    "        existing_data[\"count\"] += total_count\n",
    "    else:\n",
    "        existing_data[\"true_count\"] += total_count\n",
    "        existing_data[\"count\"] += total_count\n",
    "        \n",
    "\n",
    "    with open(json_filename, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(existing_data, json_file, indent=4)\n",
    "        \n",
    "    if total_count == 0:\n",
    "        \n",
    "        return\n",
    "    \n",
    "    # Check for abnormal & record \n",
    "    if total_count >= 1000:\n",
    "            print(f\"Abnormal count detected for {disease}: {total_count}.\")\n",
    "            with open(abnormal_file, \"a\") as ab_file:\n",
    "                ab_file.write(f\"{disease}: {total_count}\\n\")\n",
    "            return\n",
    "    \n",
    "    # Use WebEnv and QueryKey for batch fetching\n",
    "    webenv = search_results[\"WebEnv\"]\n",
    "    query_key = search_results[\"QueryKey\"]\n",
    "\n",
    "    all_data = \"\"\n",
    "    batch_size = 10000\n",
    "    \n",
    "    for start in range(0, total_count, batch_size):\n",
    "        \n",
    "        with Entrez.efetch(\n",
    "            db=\"pubmed\",\n",
    "            rettype=\"medline\",\n",
    "            retmode=\"text\",\n",
    "            retstart=start,\n",
    "            retmax=batch_size,\n",
    "            webenv=webenv,\n",
    "            query_key=query_key\n",
    "        ) as fetch_handle:\n",
    "            all_data += fetch_handle.read()\n",
    "\n",
    "    \n",
    "    # Append PubMed Search \n",
    "    #with open(filename, \"a\") as file:\n",
    "        #file.write(all_data)\n",
    "    \n",
    "    # Create separate file for recording \n",
    "    disease_dir = './pubmed_search/all_diseases'\n",
    "    os.makedirs(disease_dir, exist_ok=True)\n",
    "    disease_file = os.path.join(disease_dir, f\"{sanitize_filename(disease)}.txt\")\n",
    "    with open(disease_file, \"w\") as file:\n",
    "        file.write(all_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_file = \"./disease_json/data.json\"\n",
    "# Read Orphanet Rare Disease names \n",
    "with open(path_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    disease_data = json.load(file)\n",
    "rare_diseases = [d['Name'] for d in disease_data['disease']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_json_filename = f\"./pubmed_search/pubmed_search_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "\n",
    "\n",
    "\n",
    "for disease in rare_diseases:\n",
    "    search_and_save_all_pubmed(disease, output_filename, output_json_filename)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
